{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.tensorflow.org/?hl=pt-br\"><img src = \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/TensorFlowLogo.svg/1280px-TensorFlowLogo.svg.png\" width = 250, align = \"center\"></a>\n",
    "\n",
    "<h1 align=center><font size = 8><strong> Exemplos do livro</strong></font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Errata do livro Fundamentals of Deep Learning:<br>https://www.oreilly.com/catalog/errataunconfirmed.csp?isbn=0636920039709"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos os códigos do livro podem ser encontrados neste repositório:<br>https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Exemplo página 42 <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declarando a varíavel deep_learning como uma constante para armazenar a string abaixo\n",
    "deep_learning = tf.constant('Deep Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui instaciamos \"sess\" para ser nossa sessão\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deep Learning'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# usamos sess.run para rodar nosso tensor e decode() para tratar o dado que é do tipo binário que será decodificado\n",
    "#https://stackoverflow.com/questions/40904979/the-print-of-string-constant-is-always-attached-with-b-intensorflow\n",
    "sess.run(deep_learning).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciando uma constante \"a\" com valor númerico igual a 2\n",
    "a = tf. constant(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciando uma constante \"b\" com valor númerico igual a 3\n",
    "b = tf.constant(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos multiply sendo a operação de multiplicação de (a,b)\n",
    "multiply = tf.math.multiply(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aqui rodamos nosso grafo e vemos o resultado logo abaixo\n",
    "sess.run(multiply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo da página 46 impossibilitado de ser usado por falta do módulo \"read_data.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo página 56 mais explicativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução ao TensorFlow e ao Aprendizado de Máquina\n",
    "#### <b>Breves palabras sobre aprendizado de máquina e tensorflow<b>\n",
    "A necessidade do aprendizado de máquina surge quando temos muitos dados e queremos aprender com eles. Os dados geralmente recebem os pares de entrada e saída(isso para problemas supervisionados !) e queremos encontrar um padrão (presumimos que exista um) entre algumas entradas e saídas dadas. Em casos simples, pode haver uma relação simples entre entrada e saída, como a relação entre gramas e quilogramas, que pode ser descrita como uma função linear. Mas o aprendizado de máquina fica mais interessante e poderoso quando a relação entre entradas e saídas não é tão óbvia no começo. Por exemplo, neste tutorial, aprenderemos a mapear imagens de dígitos para seus rótulos. Muito legal, certo?\n",
    "De acordo com o website tensorflow, este é considerado o \"hello world\" da machine learning, então vamos começar.\n",
    "<br>\n",
    "#### <b>Metodologia e abordagem do problema<b>\n",
    "Um problema de aprendizado de máquina pode ser dividido em quatro partes: adquirir os dados, especificar os modelos (o tipo de relação entre entrada e saída deve procurar), treinar o computador para procurar o melhor modelo entre o conjunto de modelos que tenho a explorar e avaliar como o modelo prevê a saída em novos dados.\n",
    "<br>\n",
    "#### <b>Os Dados<b>\n",
    "A primeira parte é a essencial que é a de adquirar os dados. Esses dados são o que o computador irá analisar para aprender a encontrar padrões entre entradas e saídas. Na maioria dos casos, os dados não são fornecidos em um formato limpo e precisam ser pré-processados para que o computador saiba o que procurar (features) e o que prever (saídas). O desafio no aprendizado de máquina é extrair os recursos que têm o poder mais preditivo dos dados. <br />\n",
    "Essa área é chamada de engenharia de features e é um componente importante do conhecimento de aprendizado de máquina. A forma de aprendizado de máquina chamada deep learning reduziu a necessidade de engenharia extensiva de recursos humanos, permitindo a aprendizagem de resultados complexos a partir de dados brutos. Em nossos casos, os dados examinarão os dataset MNIST que é formado por imagens em preto e branco de números (descritos por uma matriz 28 * 28 de valores de pixel) e seus rótulos (o número que eles representam). O primeiro passo é baixar esse conjunto de dados no repositório. Felizmente, há algum código já escrito que faz esse trabalho para nós nas próximas três linhas seguintes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Dowload conluído !\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "print(\"Dowload conluído !\")\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de baixar nossos dados, queremos dividi-lo em pares de entrada e saída. A maneira mais simples de representar uma imagem em preto e branco 28 * 28 é como uma lista de 784 valores de pixels. Para a saída, cada número é descrito como um vetor do tipo\"one_hot\" (ondo o elemento é 1 se o rótulo corresponder ao elemento i). Por exemplo, 3 será semelhante a: [0,0,0,1,0,0,0,0,0,0] ( lembre-se começamos a contagem a partir de zero em python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_vector = tf.placeholder(tf.float32, [None, 784], \"image_vector\" )\n",
    "labels = tf.placeholder(tf.float32, [None, 10],\"labels\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nas duas linhas anteriores de código, dissemos ao TensorFlow que uma parte dos dados viria como um vetor de feição consistindo de um número de ponto flutuante de 784 dimensões e outra parte dos dados viria como um vetor de 10 dimensões. A parte onde se encontra \"None\" é referente ao número de instâncias do dataset, que no caso não importa qual número seja então por default colocamos \"None\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelagem parte 1: Escolhendo o modelo\n",
    "Agora, depois de fazer o download dos nossos dados, definimos o tipo de relacionamento entre os \"image_vectors\" e os \"labels\" que queremos que o computador procure. No nosso caso, o modelo é chamado de regressão logística. Na regressão logística, fazemos uma soma ponderada de cada recurso em um vetor de recurso e rótulos diferentes que correspondem a pesos diferentes em cada dimensão do vetor de recurso. Essa soma é indicativa de quanto o computador acredita que um exemplo pertence a uma classe específica. A saída desse modelo de regressão logística pode ser vista como um escore de confiança (mais precisamente, uma distribuição de probabilidade) em cada um dos diferentes tipos de classe de rótulos. Por exemplo, uma saída de [0.2,0,0.8,0,0,0,0,0,0,0] indica uma confiança de 80% de que a imagem é o número \"2\" e 20% de confiança de que a imagem é o número \"0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo escolhido !\n"
     ]
    }
   ],
   "source": [
    "# Aqui iremos chamar nosso intervalo de confiança de pred que é relativo a sua função de predição\n",
    "# 1) inference\n",
    "with tf.name_scope('hidden') as scope:\n",
    "    Weights = tf.Variable(tf.zeros([784, 10]), name = \"weight\")\n",
    "    bias = tf.Variable(tf.zeros([10]), name = \"bias\")\n",
    "    pred = tf.nn.softmax(tf.matmul(image_vector, Weights) + bias, name = \"pred\") \n",
    "print(\"Modelo escolhido !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelagem parte 2: Escolhendo a função de custo(loss)\n",
    "Agora, depois de termos definido nosso modelo para encontrar uma relação entre as entradas e as saídas, queremos encontrar uma maneira de medir o quão errado ou correto está. No nosso caso, usamos a função de custo de entropia cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Função de custo escolhida !\n"
     ]
    }
   ],
   "source": [
    "#  2) Loss\n",
    "learning_rate = 0.01\n",
    "# 3) training\n",
    "loss_function = -tf.reduce_sum(labels*tf.log(pred), name = \"loss_function\") \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_function) \n",
    "print(\"Função de custo escolhida !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de definir nossa função de custo, podemos dizer ao programa que o objetivo do processo de aprendizagem é aprender o conjunto de pesos e bias que minimizarão essa função de custo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento: Mostrando exemplos rotulados para o computador\n",
    "Depois de definir todas as variáveis, nós as inicializamos em valores aleatórios e parte do processo de treinamento é ajustar esses valores conforme o exemplo chegar. Para começar a mostrar exemplos, agora abrimos uma sessão do TensorFlow. Uma sessão do TensorFlow permite executar operações predefinidas de tensorflow com um determinado conjunto de recursos. Como o aprendizado de máquina é intensivo em computação, queremos alocar recursos apenas no momento certo, executar todos os nossos cálculos e desalocar esses recursos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicialização concluída !\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "print(\"Inicialização concluída !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui nós usamos gradiente descendente estocástico em mini-lote, onde usamoscom um mini batch_size de 100 e época de treinamento de 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parâmetros de treino escolhidos !\n"
     ]
    }
   ],
   "source": [
    "training_epochs = 51\n",
    "mini_batch_size = 100\n",
    "display_step = 10\n",
    "print(\"Parâmetros de treino escolhidos !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora nós lançamos o gráfico de execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch: 0001 the average loss is 29.860468258\n",
      "At epoch: 0011 the average loss is 19.160383248\n",
      "At epoch: 0021 the average loss is 18.351257650\n",
      "At epoch: 0031 the average loss is 18.082159883\n",
      "At epoch: 0041 the average loss is 17.804783694\n",
      "At epoch: 0051 the average loss is 17.657067384\n",
      "Training done\n",
      "Accuracyn train: 0.9335273\n",
      "Accuracyn validation: 0.9232\n",
      "Accuracyn test: 0.9207\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "# 4) evaluete\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_loss = 0.\n",
    "        total_batch_size = int(mnist.train.num_examples/mini_batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch_size):\n",
    "            batch_images, batch_labels = mnist.train.next_batch(mini_batch_size)\n",
    "            # Fit training using batch data\n",
    "            sess.run(optimizer, feed_dict={image_vector: batch_images, labels: batch_labels})\n",
    "            # Compute average loss\n",
    "            avg_loss += sess.run(loss_function, feed_dict={image_vector: batch_images, labels: batch_labels})/total_batch_size\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"At epoch:\", '%04d' % (epoch+1), \"the average loss is\", \"{:.9f}\".format(avg_loss))\n",
    "\n",
    "    print(\"Training done\")\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(labels, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracyn train:\", accuracy.eval({image_vector: mnist.train.images, labels: mnist.train.labels}))\n",
    "    print(\"Accuracyn validation:\", accuracy.eval({image_vector: mnist.validation.images, labels: mnist.validation.labels}))\n",
    "    print(\"Accuracyn test:\", accuracy.eval({image_vector: mnist.test.images, labels: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Aqui temos o exemplo do livro da página 56 do jeito que o livro mostra.<b>\n",
    "Segue o link do código no link: https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book/blob/master/archive/logistic_regression.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 60\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(x):\n",
    "    init = tf.constant_initializer(value=0)\n",
    "    W = tf.get_variable(\"W\", [784, 10],\n",
    "                         initializer=init)\n",
    "    b = tf.get_variable(\"b\", [10],\n",
    "                         initializer=init)\n",
    "    output = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "    w_hist = tf.summary.histogram(\"weights\", W)\n",
    "    b_hist = tf.summary.histogram(\"biases\", b)\n",
    "    y_hist = tf.summary.histogram(\"output\", output)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(output, y):\n",
    "    dot_product = y * tf.log(output)\n",
    "\n",
    "    # Reduction along axis 0 collapses each column into a single\n",
    "    # value, whereas reduction along axis 1 collapses each row \n",
    "    # into a single value. In general, reduction along axis i \n",
    "    # collapses the ith dimension of a tensor to size 1.\n",
    "    xentropy = -tf.reduce_sum(dot_product, reduction_indices=1)\n",
    "     \n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(cost, global_step):\n",
    "\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(output, y):\n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    tf.summary.scalar(\"validation error\", (1.0 - accuracy))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name validation error is illegal; using validation_error instead.\n",
      "WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n",
      "Epoch: 0001 cost = 1.176687779\n",
      "Validation Error: 0.15140002965927124\n",
      "Epoch: 0002 cost = 0.662704233\n",
      "Validation Error: 0.12940001487731934\n",
      "Epoch: 0003 cost = 0.550709084\n",
      "Validation Error: 0.12099999189376831\n",
      "Epoch: 0004 cost = 0.496781324\n",
      "Validation Error: 0.11360001564025879\n",
      "Epoch: 0005 cost = 0.463765688\n",
      "Validation Error: 0.1096000075340271\n",
      "Epoch: 0006 cost = 0.440919228\n",
      "Validation Error: 0.10680001974105835\n",
      "Epoch: 0007 cost = 0.423937223\n",
      "Validation Error: 0.10479998588562012\n",
      "Epoch: 0008 cost = 0.410694792\n",
      "Validation Error: 0.10280001163482666\n",
      "Epoch: 0009 cost = 0.399892179\n",
      "Validation Error: 0.10100001096725464\n",
      "Epoch: 0010 cost = 0.390973201\n",
      "Validation Error: 0.09859997034072876\n",
      "Epoch: 0011 cost = 0.383365469\n",
      "Validation Error: 0.09619998931884766\n",
      "Epoch: 0012 cost = 0.376814710\n",
      "Validation Error: 0.09600001573562622\n",
      "Epoch: 0013 cost = 0.371026240\n",
      "Validation Error: 0.0942000150680542\n",
      "Epoch: 0014 cost = 0.365912119\n",
      "Validation Error: 0.09280002117156982\n",
      "Epoch: 0015 cost = 0.361380490\n",
      "Validation Error: 0.09320002794265747\n",
      "Epoch: 0016 cost = 0.357252639\n",
      "Validation Error: 0.0899999737739563\n",
      "Epoch: 0017 cost = 0.353535599\n",
      "Validation Error: 0.08980000019073486\n",
      "Epoch: 0018 cost = 0.350174260\n",
      "Validation Error: 0.0899999737739563\n",
      "Epoch: 0019 cost = 0.347023123\n",
      "Validation Error: 0.08859997987747192\n",
      "Epoch: 0020 cost = 0.344136538\n",
      "Validation Error: 0.0878000259399414\n",
      "Epoch: 0021 cost = 0.341469252\n",
      "Validation Error: 0.08719998598098755\n",
      "Epoch: 0022 cost = 0.338940018\n",
      "Validation Error: 0.0878000259399414\n",
      "Epoch: 0023 cost = 0.336647300\n",
      "Validation Error: 0.08660000562667847\n",
      "Epoch: 0024 cost = 0.334499597\n",
      "Validation Error: 0.08639997243881226\n",
      "Epoch: 0025 cost = 0.332495577\n",
      "Validation Error: 0.08520001173019409\n",
      "Epoch: 0026 cost = 0.330499103\n",
      "Validation Error: 0.08579999208450317\n",
      "Epoch: 0027 cost = 0.328735288\n",
      "Validation Error: 0.08499997854232788\n",
      "Epoch: 0028 cost = 0.327023429\n",
      "Validation Error: 0.08539998531341553\n",
      "Epoch: 0029 cost = 0.325364761\n",
      "Validation Error: 0.08480000495910645\n",
      "Epoch: 0030 cost = 0.323832235\n",
      "Validation Error: 0.08420002460479736\n",
      "Epoch: 0031 cost = 0.322353583\n",
      "Validation Error: 0.08319997787475586\n",
      "Epoch: 0032 cost = 0.320943657\n",
      "Validation Error: 0.08480000495910645\n",
      "Epoch: 0033 cost = 0.319602088\n",
      "Validation Error: 0.08459997177124023\n",
      "Epoch: 0034 cost = 0.318345548\n",
      "Validation Error: 0.0843999981880188\n",
      "Epoch: 0035 cost = 0.317135168\n",
      "Validation Error: 0.08380001783370972\n",
      "Epoch: 0036 cost = 0.315890464\n",
      "Validation Error: 0.08279997110366821\n",
      "Epoch: 0037 cost = 0.314822022\n",
      "Validation Error: 0.0812000036239624\n",
      "Epoch: 0038 cost = 0.313785550\n",
      "Validation Error: 0.08139997720718384\n",
      "Epoch: 0039 cost = 0.312678529\n",
      "Validation Error: 0.08079999685287476\n",
      "Epoch: 0040 cost = 0.311644611\n",
      "Validation Error: 0.08099997043609619\n",
      "Epoch: 0041 cost = 0.310699940\n",
      "Validation Error: 0.08039999008178711\n",
      "Epoch: 0042 cost = 0.309813155\n",
      "Validation Error: 0.08020001649856567\n",
      "Epoch: 0043 cost = 0.308868758\n",
      "Validation Error: 0.07980000972747803\n",
      "Epoch: 0044 cost = 0.308022622\n",
      "Validation Error: 0.07980000972747803\n",
      "Epoch: 0045 cost = 0.307200604\n",
      "Validation Error: 0.07999998331069946\n",
      "Epoch: 0046 cost = 0.306394576\n",
      "Validation Error: 0.07940000295639038\n",
      "Epoch: 0047 cost = 0.305594052\n",
      "Validation Error: 0.07940000295639038\n",
      "Epoch: 0048 cost = 0.304827213\n",
      "Validation Error: 0.07819998264312744\n",
      "Epoch: 0049 cost = 0.304062263\n",
      "Validation Error: 0.07999998331069946\n",
      "Epoch: 0050 cost = 0.303339435\n",
      "Validation Error: 0.0788000226020813\n",
      "Epoch: 0051 cost = 0.302664921\n",
      "Validation Error: 0.078000009059906\n",
      "Epoch: 0052 cost = 0.301956745\n",
      "Validation Error: 0.078000009059906\n",
      "Epoch: 0053 cost = 0.301328874\n",
      "Validation Error: 0.07859998941421509\n",
      "Epoch: 0054 cost = 0.300689785\n",
      "Validation Error: 0.07760000228881836\n",
      "Epoch: 0055 cost = 0.300045443\n",
      "Validation Error: 0.078000009059906\n",
      "Epoch: 0056 cost = 0.299460620\n",
      "Validation Error: 0.07700002193450928\n",
      "Epoch: 0057 cost = 0.298821537\n",
      "Validation Error: 0.07819998264312744\n",
      "Epoch: 0058 cost = 0.298271579\n",
      "Validation Error: 0.0777999758720398\n",
      "Epoch: 0059 cost = 0.297638902\n",
      "Validation Error: 0.078000009059906\n",
      "Epoch: 0060 cost = 0.297094286\n",
      "Validation Error: 0.078000009059906\n",
      "Optimization Finished!\n",
      "Test Accuracy: 0.9196\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        x = tf.placeholder(\"float\", [None, 784]) # mnist data image of shape 28*28=784\n",
    "        y = tf.placeholder(\"float\", [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "\n",
    "        output = inference(x)\n",
    "\n",
    "        cost = loss(output, y)\n",
    "\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "        train_op = training(cost, global_step)\n",
    "\n",
    "        eval_op = evaluate(output, y)\n",
    "\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        sess = tf.Session()\n",
    "\n",
    "        file_writer = tf.summary.FileWriter(\"logistic_logs/\",\n",
    "                                            graph_def=sess.graph_def)\n",
    "\n",
    "        \n",
    "        init_op = tf.initialize_all_variables()\n",
    "\n",
    "        sess.run(init_op)\n",
    "\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(training_epochs):\n",
    "\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(mnist.train.num_examples/batch_size)\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                minibatch_x, minibatch_y = mnist.train.next_batch(batch_size)\n",
    "                # Fit training using batch data\n",
    "                sess.run(train_op, feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "                # Compute average loss\n",
    "                avg_cost += sess.run(cost, feed_dict={x: minibatch_x, y: minibatch_y})/total_batch\n",
    "            # Display logs per epoch step\n",
    "            if epoch % display_step == 0:\n",
    "                print(\"Epoch:\", '%04d' % (epoch+1), \"cost =\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "                accuracy = sess.run(eval_op, feed_dict={x: mnist.validation.images, y: mnist.validation.labels})\n",
    "\n",
    "                print(\"Validation Error:\", (1 - accuracy))\n",
    "\n",
    "                summary_str = sess.run(summary_op, feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "                file_writer.add_summary(summary_str, sess.run(global_step))\n",
    "\n",
    "                saver.save(sess, \"logistic_logs/model-checkpoint\", global_step=global_step)\n",
    "\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "\n",
    "        accuracy = sess.run(eval_op, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "\n",
    "        print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
